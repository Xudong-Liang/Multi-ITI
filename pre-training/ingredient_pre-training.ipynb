{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No normalization for SPS\n",
      "WARNING:root:No normalization for BCUT2D_MWHI\n",
      "WARNING:root:No normalization for BCUT2D_MWLOW\n",
      "WARNING:root:No normalization for BCUT2D_CHGHI\n",
      "WARNING:root:No normalization for BCUT2D_CHGLO\n",
      "WARNING:root:No normalization for BCUT2D_LOGPHI\n",
      "WARNING:root:No normalization for BCUT2D_LOGPLOW\n",
      "WARNING:root:No normalization for BCUT2D_MRHI\n",
      "WARNING:root:No normalization for BCUT2D_MRLOW\n",
      "WARNING:root:No normalization for AvgIpc\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "from src.data.descriptors.rdNormalizedDescriptors import RDKit2DNormalized\n",
    "from src.data.featurizer import smiles_to_graph_tune\n",
    "from scipy import sparse as sp\n",
    "from multiprocessing import Pool\n",
    "import dgl.backend as F\n",
    "from dgl.data.utils import load_graphs\n",
    "from dgl.data.utils import save_graphs\n",
    "from dgllife.utils.io import pmap\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils import set_random_seed\n",
    "from src.data.featurizer import Vocab, N_ATOM_TYPES, N_BOND_TYPES\n",
    "from src.data.finetune_dataset import MoleculeDataset\n",
    "from src.data.collator import Collator_tune\n",
    "from src.model.light import LiGhTPredictor as LiGhT\n",
    "from src.model_config import config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load smiles file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/smiles/ori_smiles.csv')\n",
    "smiless = df.smiles.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Generate molecular graph file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 1184 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done 1222 out of 1237 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 1237 out of 1237 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving graphs\n"
     ]
    }
   ],
   "source": [
    "n_jobs = 8\n",
    "task_names = df.columns.drop(['smiles']).tolist()\n",
    "print('constructing graphs')\n",
    "graphs = pmap(smiles_to_graph_tune,\n",
    "              smiless,\n",
    "              max_length=5,\n",
    "              n_virtual_nodes=2,\n",
    "              n_jobs=n_jobs)\n",
    "valid_ids = []\n",
    "valid_graphs = []\n",
    "# Index without smiles: [935, 950, 975, 990, 1039, 1080]\n",
    "none_smiles_index = []\n",
    "for i, g in enumerate(graphs):\n",
    "    if g is not None:\n",
    "        valid_ids.append(i)\n",
    "        valid_graphs.append(g)\n",
    "    else:\n",
    "        none_smiles_index.append(i)\n",
    "_label_values = df[task_names].values\n",
    "labels = F.zerocopy_from_numpy(\n",
    "    _label_values.astype(np.float32))[valid_ids]\n",
    "print('saving graphs')\n",
    "save_graphs('datasets/smiles/smiles_5.pkl', valid_graphs, labels={'labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 1231\n",
      "Graph(num_nodes=31, num_edges=563,\n",
      "      ndata_schemes={'begin_end': Scheme(shape=(2, 137), dtype=torch.float32), 'vavn': Scheme(shape=(), dtype=torch.int64), 'edge': Scheme(shape=(14,), dtype=torch.float32)}\n",
      "      edata_schemes={'path': Scheme(shape=(5,), dtype=torch.int64), 'lgp': Scheme(shape=(), dtype=torch.uint8), 'mgp': Scheme(shape=(), dtype=torch.uint8), 'vp': Scheme(shape=(), dtype=torch.uint8), 'sl': Scheme(shape=(), dtype=torch.uint8)})\n",
      "{'labels': tensor([], size=(1231, 0))}\n"
     ]
    }
   ],
   "source": [
    "graphs, label_dict = load_graphs('datasets/smiles/smiles_5.pkl')\n",
    "\n",
    "print(f\"Number of graphs: {len(graphs)}\")\n",
    "\n",
    "# Print detailed information of the first graph\n",
    "if len(graphs) > 0:\n",
    "    print(graphs[0])\n",
    "\n",
    "# Print the content of label_dict\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Generate fingerprint file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting fingerprints\n",
      "saving fingerprints\n"
     ]
    }
   ],
   "source": [
    "print('extracting fingerprints')\n",
    "FP_list = []\n",
    "for smiles in smiless:\n",
    "    if smiles != 'None':\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=False)\n",
    "        fp = list(Chem.RDKFingerprint(mol, minPath=1, maxPath=7, fpSize=512))\n",
    "    FP_list.append(fp)\n",
    "\n",
    "FP_arr = np.array(FP_list)\n",
    "FP_sp_mat = sp.csc_matrix(FP_arr)\n",
    "print('saving fingerprints')\n",
    "sp.save_npz('datasets/smiles/rdkfp1-7_512.npz', FP_sp_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Generate molecular descriptor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "print('extracting molecular descriptors')\n",
    "n_jobs = 8\n",
    "generator = RDKit2DNormalized()\n",
    "# 201-dimension vector, the first dimension is a boolean value, and the next 200 dimensions are floating number.\n",
    "features_map = Pool(n_jobs).imap(generator.process, smiless)\n",
    "features_list = list(features_map)\n",
    "# 处理空值\n",
    "features = []\n",
    "for i in features_list:\n",
    "    if i is not None:\n",
    "        features.append(i)\n",
    "# none_indices = [i for i, x in enumerate(features_list) if x is None]\n",
    "arr = np.array(features)\n",
    "np.savez_compressed('datasets/smiles/molecular_descriptors.npz',md=arr[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(22,1)\n",
    "config = config_dict['base']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab = Vocab(N_ATOM_TYPES, N_BOND_TYPES)\n",
    "collator = Collator_tune(config['path_length'])\n",
    "mol_dataset = MoleculeDataset(root_path='datasets', dataset = 'smiles', dataset_type=None)\n",
    "loader = DataLoader(mol_dataset, batch_size=32, shuffle=False, num_workers=4, drop_last=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load pre-trained KPGT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LiGhT(\n",
    "    d_node_feats=config['d_node_feats'],\n",
    "    d_edge_feats=config['d_edge_feats'],\n",
    "    d_g_feats=config['d_g_feats'],\n",
    "    d_hpath_ratio=config['d_hpath_ratio'],\n",
    "    n_mol_layers=config['n_mol_layers'],\n",
    "    path_length=config['path_length'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_ffn_dense_layers=config['n_ffn_dense_layers'],\n",
    "    input_drop=0,\n",
    "    attn_drop=0,\n",
    "    feat_drop=0,\n",
    "    n_node_types=vocab.vocab_size\n",
    "    ).to(device)\n",
    "\n",
    "model.load_state_dict({k.replace('module.',''):v for k,v in torch.load('models/KPGT/base.pth').items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_list = []\n",
    "for batch_idx, batched_data in enumerate(loader):\n",
    "    (_, g, ecfp, md, labels) = batched_data\n",
    "    ecfp = ecfp.to(device)\n",
    "    md = md.to(device)\n",
    "    g = g.to(device)\n",
    "    fps = model.generate_fps(g, ecfp, md)\n",
    "    fps_list.extend(fps.detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Randomly generate embeddings for components without smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the indices of the data without SMILES\n",
    "none_indices = [935, 950, 975, 990, 1039, 1080]\n",
    "\n",
    "# Generate random embeddings for the data without SMILES\n",
    "random_fps = np.random.normal(size=(len(none_indices), 2304)).tolist()\n",
    "\n",
    "# Insert the random embeddings into the fps_list at the correct indices\n",
    "for idx, random_fp in zip(none_indices, random_fps):\n",
    "    fps_list.insert(idx, random_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Generate embeddings files (.npz and .csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('datasets/smiles/kpgt_embeddings.npz', fps=np.array(fps_list))\n",
    "print(f\"The extracted features were saving at 'datasets/smiles/kpgt_embeddings.npz'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also provide embeddings files in \".csv\" format\n",
    "ac = np.load('datasets/smiles/kpgt_embeddings.npz')\n",
    "arr = ac['fps']\n",
    "df = pd.DataFrame(arr)\n",
    "df.to_csv('datasets/smiles/smiles_embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
